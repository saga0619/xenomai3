From ade5db5a61fcb39eb424e4b5565eb7f319952a06 Mon Sep 17 00:00:00 2001
From: Gino Strobbe <G.Strobbe@telenet.be>
Date: Sat, 1 Jun 2024 08:21:39 +0200
Subject: [PATCH] [v1 02/12] e1000e: Remove functions

Signed-off-by: Gino Strobbe <G.Strobbe@telenet.be>
---
 kernel/drivers/net/drivers/e1000e/netdev.c | 1476 +-------------------
 1 file changed, 32 insertions(+), 1444 deletions(-)

diff --git a/kernel/drivers/net/drivers/e1000e/netdev.c b/kernel/drivers/net/drivers/e1000e/netdev.c
index 3692fce20..513126d9e 100644
--- a/kernel/drivers/net/drivers/e1000e/netdev.c
+++ b/kernel/drivers/net/drivers/e1000e/netdev.c
@@ -541,30 +541,6 @@ static void e1000e_rx_hwtstamp(struct e1000_adapter *adapter, u32 status,
 	adapter->flags2 &= ~FLAG2_CHECK_RX_HWTSTAMP;
 }
 
-/**
- * e1000_receive_skb - helper function to handle Rx indications
- * @adapter: board private structure
- * @netdev: pointer to netdev struct
- * @staterr: descriptor extended error and status field as written by hardware
- * @vlan: descriptor vlan field as written by hardware (no le/be conversion)
- * @skb: pointer to sk_buff to be indicated to stack
- **/
-static void e1000_receive_skb(struct e1000_adapter *adapter,
-			      struct net_device *netdev, struct sk_buff *skb,
-			      u32 staterr, __le16 vlan)
-{
-	u16 tag = le16_to_cpu(vlan);
-
-	e1000e_rx_hwtstamp(adapter, staterr, skb);
-
-	skb->protocol = eth_type_trans(skb, netdev);
-
-	if (staterr & E1000_RXD_STAT_VP)
-		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
-
-	napi_gro_receive(&adapter->napi, skb);
-}
-
 /**
  * e1000_rx_checksum - Receive Checksum Offload
  * @adapter: board private structure
@@ -707,201 +683,6 @@ map_skb:
 	rx_ring->next_to_use = i;
 }
 
-/**
- * e1000_alloc_rx_buffers_ps - Replace used receive buffers; packet split
- * @rx_ring: Rx descriptor ring
- * @cleaned_count: number to reallocate
- * @gfp: flags for allocation
- **/
-static void e1000_alloc_rx_buffers_ps(struct e1000_ring *rx_ring,
-				      int cleaned_count, gfp_t gfp)
-{
-	struct e1000_adapter *adapter = rx_ring->adapter;
-	struct net_device *netdev = adapter->netdev;
-	struct pci_dev *pdev = adapter->pdev;
-	union e1000_rx_desc_packet_split *rx_desc;
-	struct e1000_buffer *buffer_info;
-	struct e1000_ps_page *ps_page;
-	struct sk_buff *skb;
-	unsigned int i, j;
-
-	i = rx_ring->next_to_use;
-	buffer_info = &rx_ring->buffer_info[i];
-
-	while (cleaned_count--) {
-		rx_desc = E1000_RX_DESC_PS(*rx_ring, i);
-
-		for (j = 0; j < PS_PAGE_BUFFERS; j++) {
-			ps_page = &buffer_info->ps_pages[j];
-			if (j >= adapter->rx_ps_pages) {
-				/* all unused desc entries get hw null ptr */
-				rx_desc->read.buffer_addr[j + 1] =
-				    ~cpu_to_le64(0);
-				continue;
-			}
-			if (!ps_page->page) {
-				ps_page->page = alloc_page(gfp);
-				if (!ps_page->page) {
-					adapter->alloc_rx_buff_failed++;
-					goto no_buffers;
-				}
-				ps_page->dma = dma_map_page(&pdev->dev,
-							    ps_page->page,
-							    0, PAGE_SIZE,
-							    DMA_FROM_DEVICE);
-				if (dma_mapping_error(&pdev->dev,
-						      ps_page->dma)) {
-					dev_err(&adapter->pdev->dev,
-						"Rx DMA page map failed\n");
-					adapter->rx_dma_failed++;
-					goto no_buffers;
-				}
-			}
-			/* Refresh the desc even if buffer_addrs
-			 * didn't change because each write-back
-			 * erases this info.
-			 */
-			rx_desc->read.buffer_addr[j + 1] =
-			    cpu_to_le64(ps_page->dma);
-		}
-
-		skb = __netdev_alloc_skb_ip_align(netdev, adapter->rx_ps_bsize0,
-						  gfp);
-
-		if (!skb) {
-			adapter->alloc_rx_buff_failed++;
-			break;
-		}
-
-		buffer_info->skb = skb;
-		buffer_info->dma = dma_map_single(&pdev->dev, skb->data,
-						  adapter->rx_ps_bsize0,
-						  DMA_FROM_DEVICE);
-		if (dma_mapping_error(&pdev->dev, buffer_info->dma)) {
-			dev_err(&pdev->dev, "Rx DMA map failed\n");
-			adapter->rx_dma_failed++;
-			/* cleanup skb */
-			dev_kfree_skb_any(skb);
-			buffer_info->skb = NULL;
-			break;
-		}
-
-		rx_desc->read.buffer_addr[0] = cpu_to_le64(buffer_info->dma);
-
-		if (unlikely(!(i & (E1000_RX_BUFFER_WRITE - 1)))) {
-			/* Force memory writes to complete before letting h/w
-			 * know there are new descriptors to fetch.  (Only
-			 * applicable for weak-ordered memory model archs,
-			 * such as IA-64).
-			 */
-			wmb();
-			if (adapter->flags2 & FLAG2_PCIM2PCI_ARBITER_WA)
-				e1000e_update_rdt_wa(rx_ring, i << 1);
-			else
-				writel(i << 1, rx_ring->tail);
-		}
-
-		i++;
-		if (i == rx_ring->count)
-			i = 0;
-		buffer_info = &rx_ring->buffer_info[i];
-	}
-
-no_buffers:
-	rx_ring->next_to_use = i;
-}
-
-/**
- * e1000_alloc_jumbo_rx_buffers - Replace used jumbo receive buffers
- * @rx_ring: Rx descriptor ring
- * @cleaned_count: number of buffers to allocate this pass
- * @gfp: flags for allocation
- **/
-
-static void e1000_alloc_jumbo_rx_buffers(struct e1000_ring *rx_ring,
-					 int cleaned_count, gfp_t gfp)
-{
-	struct e1000_adapter *adapter = rx_ring->adapter;
-	struct net_device *netdev = adapter->netdev;
-	struct pci_dev *pdev = adapter->pdev;
-	union e1000_rx_desc_extended *rx_desc;
-	struct e1000_buffer *buffer_info;
-	struct sk_buff *skb;
-	unsigned int i;
-	unsigned int bufsz = 256 - 16;	/* for skb_reserve */
-
-	i = rx_ring->next_to_use;
-	buffer_info = &rx_ring->buffer_info[i];
-
-	while (cleaned_count--) {
-		skb = buffer_info->skb;
-		if (skb) {
-			skb_trim(skb, 0);
-			goto check_page;
-		}
-
-		skb = __netdev_alloc_skb_ip_align(netdev, bufsz, gfp);
-		if (unlikely(!skb)) {
-			/* Better luck next round */
-			adapter->alloc_rx_buff_failed++;
-			break;
-		}
-
-		buffer_info->skb = skb;
-check_page:
-		/* allocate a new page if necessary */
-		if (!buffer_info->page) {
-			buffer_info->page = alloc_page(gfp);
-			if (unlikely(!buffer_info->page)) {
-				adapter->alloc_rx_buff_failed++;
-				break;
-			}
-		}
-
-		if (!buffer_info->dma) {
-			buffer_info->dma = dma_map_page(&pdev->dev,
-							buffer_info->page, 0,
-							PAGE_SIZE,
-							DMA_FROM_DEVICE);
-			if (dma_mapping_error(&pdev->dev, buffer_info->dma)) {
-				adapter->alloc_rx_buff_failed++;
-				break;
-			}
-		}
-
-		rx_desc = E1000_RX_DESC_EXT(*rx_ring, i);
-		rx_desc->read.buffer_addr = cpu_to_le64(buffer_info->dma);
-
-		if (unlikely(++i == rx_ring->count))
-			i = 0;
-		buffer_info = &rx_ring->buffer_info[i];
-	}
-
-	if (likely(rx_ring->next_to_use != i)) {
-		rx_ring->next_to_use = i;
-		if (unlikely(i-- == 0))
-			i = (rx_ring->count - 1);
-
-		/* Force memory writes to complete before letting h/w
-		 * know there are new descriptors to fetch.  (Only
-		 * applicable for weak-ordered memory model archs,
-		 * such as IA-64).
-		 */
-		wmb();
-		if (adapter->flags2 & FLAG2_PCIM2PCI_ARBITER_WA)
-			e1000e_update_rdt_wa(rx_ring, i);
-		else
-			writel(i, rx_ring->tail);
-	}
-}
-
-static inline void e1000_rx_hash(struct net_device *netdev, __le32 rss,
-				 struct sk_buff *skb)
-{
-	if (netdev->features & NETIF_F_RXHASH)
-		skb_set_hash(skb, le32_to_cpu(rss), PKT_HASH_TYPE_L3);
-}
-
 /**
  * e1000_clean_rx_irq - Send received data up the network stack
  * @rx_ring: Rx descriptor ring
@@ -1027,8 +808,6 @@ static bool e1000_clean_rx_irq(struct e1000_ring *rx_ring, int *work_done,
 		/* Receive Checksum Offload */
 		e1000_rx_checksum(adapter, staterr, skb);
 
-		e1000_rx_hash(netdev, rx_desc->wb.lower.hi_dword.rss, skb);
-
 		e1000_receive_skb(adapter, netdev, skb, staterr,
 				  rx_desc->wb.upper.vlan);
 
@@ -1084,83 +863,6 @@ static void e1000_put_txbuf(struct e1000_ring *tx_ring,
 	buffer_info->time_stamp = 0;
 }
 
-static void e1000_print_hw_hang(struct work_struct *work)
-{
-	struct e1000_adapter *adapter = container_of(work,
-						     struct e1000_adapter,
-						     print_hang_task);
-	struct net_device *netdev = adapter->netdev;
-	struct e1000_ring *tx_ring = adapter->tx_ring;
-	unsigned int i = tx_ring->next_to_clean;
-	unsigned int eop = tx_ring->buffer_info[i].next_to_watch;
-	struct e1000_tx_desc *eop_desc = E1000_TX_DESC(*tx_ring, eop);
-	struct e1000_hw *hw = &adapter->hw;
-	u16 phy_status, phy_1000t_status, phy_ext_status;
-	u16 pci_status;
-
-	if (test_bit(__E1000_DOWN, &adapter->state))
-		return;
-
-	if (!adapter->tx_hang_recheck && (adapter->flags2 & FLAG2_DMA_BURST)) {
-		/* May be block on write-back, flush and detect again
-		 * flush pending descriptor writebacks to memory
-		 */
-		ew32(TIDV, adapter->tx_int_delay | E1000_TIDV_FPD);
-		/* execute the writes immediately */
-		e1e_flush();
-		/* Due to rare timing issues, write to TIDV again to ensure
-		 * the write is successful
-		 */
-		ew32(TIDV, adapter->tx_int_delay | E1000_TIDV_FPD);
-		/* execute the writes immediately */
-		e1e_flush();
-		adapter->tx_hang_recheck = true;
-		return;
-	}
-	adapter->tx_hang_recheck = false;
-
-	if (er32(TDH(0)) == er32(TDT(0))) {
-		e_dbg("false hang detected, ignoring\n");
-		return;
-	}
-
-	/* Real hang detected */
-	netif_stop_queue(netdev);
-
-	e1e_rphy(hw, MII_BMSR, &phy_status);
-	e1e_rphy(hw, MII_STAT1000, &phy_1000t_status);
-	e1e_rphy(hw, MII_ESTATUS, &phy_ext_status);
-
-	pci_read_config_word(adapter->pdev, PCI_STATUS, &pci_status);
-
-	/* detected Hardware unit hang */
-	e_err("Detected Hardware Unit Hang:\n"
-	      "  TDH                  <%x>\n"
-	      "  TDT                  <%x>\n"
-	      "  next_to_use          <%x>\n"
-	      "  next_to_clean        <%x>\n"
-	      "buffer_info[next_to_clean]:\n"
-	      "  time_stamp           <%lx>\n"
-	      "  next_to_watch        <%x>\n"
-	      "  jiffies              <%lx>\n"
-	      "  next_to_watch.status <%x>\n"
-	      "MAC Status             <%x>\n"
-	      "PHY Status             <%x>\n"
-	      "PHY 1000BASE-T Status  <%x>\n"
-	      "PHY Extended Status    <%x>\n"
-	      "PCI Status             <%x>\n",
-	      readl(tx_ring->head), readl(tx_ring->tail), tx_ring->next_to_use,
-	      tx_ring->next_to_clean, tx_ring->buffer_info[eop].time_stamp,
-	      eop, jiffies, eop_desc->upper.fields.status, er32(STATUS),
-	      phy_status, phy_1000t_status, phy_ext_status, pci_status);
-
-	e1000e_dump(adapter);
-
-	/* Suggest workaround for known h/w issue */
-	if ((hw->mac.type == e1000_pchlan) && (er32(CTRL) & E1000_CTRL_TFCE))
-		e_err("Try turning off Tx pause (flow control) via ethtool\n");
-}
-
 /**
  * e1000e_tx_hwtstamp_work - check for Tx time stamp
  * @work: pointer to work struct
@@ -1297,372 +999,7 @@ static bool e1000_clean_tx_irq(struct e1000_ring *tx_ring)
 	adapter->total_tx_packets += total_tx_packets;
 	return count < tx_ring->count;
 }
-
-/**
- * e1000_clean_rx_irq_ps - Send received data up the network stack; packet split
- * @rx_ring: Rx descriptor ring
- * @work_done: output parameter for indicating completed work
- * @work_to_do: how many packets we can clean
- *
- * the return value indicates whether actual cleaning was done, there
- * is no guarantee that everything was cleaned
- **/
-static bool e1000_clean_rx_irq_ps(struct e1000_ring *rx_ring, int *work_done,
-				  int work_to_do)
-{
-	struct e1000_adapter *adapter = rx_ring->adapter;
-	struct e1000_hw *hw = &adapter->hw;
-	union e1000_rx_desc_packet_split *rx_desc, *next_rxd;
-	struct net_device *netdev = adapter->netdev;
-	struct pci_dev *pdev = adapter->pdev;
-	struct e1000_buffer *buffer_info, *next_buffer;
-	struct e1000_ps_page *ps_page;
-	struct sk_buff *skb;
-	unsigned int i, j;
-	u32 length, staterr;
-	int cleaned_count = 0;
-	bool cleaned = false;
-	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
-
-	i = rx_ring->next_to_clean;
-	rx_desc = E1000_RX_DESC_PS(*rx_ring, i);
-	staterr = le32_to_cpu(rx_desc->wb.middle.status_error);
-	buffer_info = &rx_ring->buffer_info[i];
-
-	while (staterr & E1000_RXD_STAT_DD) {
-		if (*work_done >= work_to_do)
-			break;
-		(*work_done)++;
-		skb = buffer_info->skb;
-		dma_rmb();	/* read descriptor and rx_buffer_info after status DD */
-
-		/* in the packet split case this is header only */
-		prefetch(skb->data - NET_IP_ALIGN);
-
-		i++;
-		if (i == rx_ring->count)
-			i = 0;
-		next_rxd = E1000_RX_DESC_PS(*rx_ring, i);
-		prefetch(next_rxd);
-
-		next_buffer = &rx_ring->buffer_info[i];
-
-		cleaned = true;
-		cleaned_count++;
-		dma_unmap_single(&pdev->dev, buffer_info->dma,
-				 adapter->rx_ps_bsize0, DMA_FROM_DEVICE);
-		buffer_info->dma = 0;
-
-		/* see !EOP comment in other Rx routine */
-		if (!(staterr & E1000_RXD_STAT_EOP))
-			adapter->flags2 |= FLAG2_IS_DISCARDING;
-
-		if (adapter->flags2 & FLAG2_IS_DISCARDING) {
-			e_dbg("Packet Split buffers didn't pick up the full packet\n");
-			dev_kfree_skb_irq(skb);
-			if (staterr & E1000_RXD_STAT_EOP)
-				adapter->flags2 &= ~FLAG2_IS_DISCARDING;
-			goto next_desc;
-		}
-
-		if (unlikely((staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK) &&
-			     !(netdev->features & NETIF_F_RXALL))) {
-			dev_kfree_skb_irq(skb);
-			goto next_desc;
-		}
-
-		length = le16_to_cpu(rx_desc->wb.middle.length0);
-
-		if (!length) {
-			e_dbg("Last part of the packet spanning multiple descriptors\n");
-			dev_kfree_skb_irq(skb);
-			goto next_desc;
-		}
-
-		/* Good Receive */
-		skb_put(skb, length);
-
-		{
-			/* this looks ugly, but it seems compiler issues make
-			 * it more efficient than reusing j
-			 */
-			int l1 = le16_to_cpu(rx_desc->wb.upper.length[0]);
-
-			/* page alloc/put takes too long and effects small
-			 * packet throughput, so unsplit small packets and
-			 * save the alloc/put
-			 */
-			if (l1 && (l1 <= copybreak) &&
-			    ((length + l1) <= adapter->rx_ps_bsize0)) {
-				ps_page = &buffer_info->ps_pages[0];
-
-				dma_sync_single_for_cpu(&pdev->dev,
-							ps_page->dma,
-							PAGE_SIZE,
-							DMA_FROM_DEVICE);
-				memcpy(skb_tail_pointer(skb),
-				       page_address(ps_page->page), l1);
-				dma_sync_single_for_device(&pdev->dev,
-							   ps_page->dma,
-							   PAGE_SIZE,
-							   DMA_FROM_DEVICE);
-
-				/* remove the CRC */
-				if (!(adapter->flags2 & FLAG2_CRC_STRIPPING)) {
-					if (!(netdev->features & NETIF_F_RXFCS))
-						l1 -= 4;
-				}
-
-				skb_put(skb, l1);
-				goto copydone;
-			}	/* if */
-		}
-
-		for (j = 0; j < PS_PAGE_BUFFERS; j++) {
-			length = le16_to_cpu(rx_desc->wb.upper.length[j]);
-			if (!length)
-				break;
-
-			ps_page = &buffer_info->ps_pages[j];
-			dma_unmap_page(&pdev->dev, ps_page->dma, PAGE_SIZE,
-				       DMA_FROM_DEVICE);
-			ps_page->dma = 0;
-			skb_fill_page_desc(skb, j, ps_page->page, 0, length);
-			ps_page->page = NULL;
-			skb->len += length;
-			skb->data_len += length;
-			skb->truesize += PAGE_SIZE;
-		}
-
-		/* strip the ethernet crc, problem is we're using pages now so
-		 * this whole operation can get a little cpu intensive
-		 */
-		if (!(adapter->flags2 & FLAG2_CRC_STRIPPING)) {
-			if (!(netdev->features & NETIF_F_RXFCS))
-				pskb_trim(skb, skb->len - 4);
-		}
-
-copydone:
-		total_rx_bytes += skb->len;
-		total_rx_packets++;
-
-		e1000_rx_checksum(adapter, staterr, skb);
-
-		e1000_rx_hash(netdev, rx_desc->wb.lower.hi_dword.rss, skb);
-
-		if (rx_desc->wb.upper.header_status &
-		    cpu_to_le16(E1000_RXDPS_HDRSTAT_HDRSP))
-			adapter->rx_hdr_split++;
-
-		e1000_receive_skb(adapter, netdev, skb, staterr,
-				  rx_desc->wb.middle.vlan);
-
-next_desc:
-		rx_desc->wb.middle.status_error &= cpu_to_le32(~0xFF);
-		buffer_info->skb = NULL;
-
-		/* return some buffers to hardware, one at a time is too slow */
-		if (cleaned_count >= E1000_RX_BUFFER_WRITE) {
-			adapter->alloc_rx_buf(rx_ring, cleaned_count,
-					      GFP_ATOMIC);
-			cleaned_count = 0;
-		}
-
-		/* use prefetched values */
-		rx_desc = next_rxd;
-		buffer_info = next_buffer;
-
-		staterr = le32_to_cpu(rx_desc->wb.middle.status_error);
-	}
-	rx_ring->next_to_clean = i;
-
-	cleaned_count = e1000_desc_unused(rx_ring);
-	if (cleaned_count)
-		adapter->alloc_rx_buf(rx_ring, cleaned_count, GFP_ATOMIC);
-
-	adapter->total_rx_bytes += total_rx_bytes;
-	adapter->total_rx_packets += total_rx_packets;
-	return cleaned;
-}
-
-static void e1000_consume_page(struct e1000_buffer *bi, struct sk_buff *skb,
-			       u16 length)
-{
-	bi->page = NULL;
-	skb->len += length;
-	skb->data_len += length;
-	skb->truesize += PAGE_SIZE;
-}
-
-/**
- * e1000_clean_jumbo_rx_irq - Send received data up the network stack; legacy
- * @rx_ring: Rx descriptor ring
- * @work_done: output parameter for indicating completed work
- * @work_to_do: how many packets we can clean
- *
- * the return value indicates whether actual cleaning was done, there
- * is no guarantee that everything was cleaned
- **/
-static bool e1000_clean_jumbo_rx_irq(struct e1000_ring *rx_ring, int *work_done,
-				     int work_to_do)
-{
-	struct e1000_adapter *adapter = rx_ring->adapter;
-	struct net_device *netdev = adapter->netdev;
-	struct pci_dev *pdev = adapter->pdev;
-	union e1000_rx_desc_extended *rx_desc, *next_rxd;
-	struct e1000_buffer *buffer_info, *next_buffer;
-	u32 length, staterr;
-	unsigned int i;
-	int cleaned_count = 0;
-	bool cleaned = false;
-	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
-	struct skb_shared_info *shinfo;
-
-	i = rx_ring->next_to_clean;
-	rx_desc = E1000_RX_DESC_EXT(*rx_ring, i);
-	staterr = le32_to_cpu(rx_desc->wb.upper.status_error);
-	buffer_info = &rx_ring->buffer_info[i];
-
-	while (staterr & E1000_RXD_STAT_DD) {
-		struct sk_buff *skb;
-
-		if (*work_done >= work_to_do)
-			break;
-		(*work_done)++;
-		dma_rmb();	/* read descriptor and rx_buffer_info after status DD */
-
-		skb = buffer_info->skb;
-		buffer_info->skb = NULL;
-
-		++i;
-		if (i == rx_ring->count)
-			i = 0;
-		next_rxd = E1000_RX_DESC_EXT(*rx_ring, i);
-		prefetch(next_rxd);
-
-		next_buffer = &rx_ring->buffer_info[i];
-
-		cleaned = true;
-		cleaned_count++;
-		dma_unmap_page(&pdev->dev, buffer_info->dma, PAGE_SIZE,
-			       DMA_FROM_DEVICE);
-		buffer_info->dma = 0;
-
-		length = le16_to_cpu(rx_desc->wb.upper.length);
-
-		/* errors is only valid for DD + EOP descriptors */
-		if (unlikely((staterr & E1000_RXD_STAT_EOP) &&
-			     ((staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK) &&
-			      !(netdev->features & NETIF_F_RXALL)))) {
-			/* recycle both page and skb */
-			buffer_info->skb = skb;
-			/* an error means any chain goes out the window too */
-			if (rx_ring->rx_skb_top)
-				dev_kfree_skb_irq(rx_ring->rx_skb_top);
-			rx_ring->rx_skb_top = NULL;
-			goto next_desc;
-		}
 #define rxtop (rx_ring->rx_skb_top)
-		if (!(staterr & E1000_RXD_STAT_EOP)) {
-			/* this descriptor is only the beginning (or middle) */
-			if (!rxtop) {
-				/* this is the beginning of a chain */
-				rxtop = skb;
-				skb_fill_page_desc(rxtop, 0, buffer_info->page,
-						   0, length);
-			} else {
-				/* this is the middle of a chain */
-				shinfo = skb_shinfo(rxtop);
-				skb_fill_page_desc(rxtop, shinfo->nr_frags,
-						   buffer_info->page, 0,
-						   length);
-				/* re-use the skb, only consumed the page */
-				buffer_info->skb = skb;
-			}
-			e1000_consume_page(buffer_info, rxtop, length);
-			goto next_desc;
-		} else {
-			if (rxtop) {
-				/* end of the chain */
-				shinfo = skb_shinfo(rxtop);
-				skb_fill_page_desc(rxtop, shinfo->nr_frags,
-						   buffer_info->page, 0,
-						   length);
-				/* re-use the current skb, we only consumed the
-				 * page
-				 */
-				buffer_info->skb = skb;
-				skb = rxtop;
-				rxtop = NULL;
-				e1000_consume_page(buffer_info, skb, length);
-			} else {
-				/* no chain, got EOP, this buf is the packet
-				 * copybreak to save the put_page/alloc_page
-				 */
-				if (length <= copybreak &&
-				    skb_tailroom(skb) >= length) {
-					memcpy(skb_tail_pointer(skb),
-					       page_address(buffer_info->page),
-					       length);
-					/* re-use the page, so don't erase
-					 * buffer_info->page
-					 */
-					skb_put(skb, length);
-				} else {
-					skb_fill_page_desc(skb, 0,
-							   buffer_info->page, 0,
-							   length);
-					e1000_consume_page(buffer_info, skb,
-							   length);
-				}
-			}
-		}
-
-		/* Receive Checksum Offload */
-		e1000_rx_checksum(adapter, staterr, skb);
-
-		e1000_rx_hash(netdev, rx_desc->wb.lower.hi_dword.rss, skb);
-
-		/* probably a little skewed due to removing CRC */
-		total_rx_bytes += skb->len;
-		total_rx_packets++;
-
-		/* eth type trans needs skb->data to point to something */
-		if (!pskb_may_pull(skb, ETH_HLEN)) {
-			e_err("pskb_may_pull failed.\n");
-			dev_kfree_skb_irq(skb);
-			goto next_desc;
-		}
-
-		e1000_receive_skb(adapter, netdev, skb, staterr,
-				  rx_desc->wb.upper.vlan);
-
-next_desc:
-		rx_desc->wb.upper.status_error &= cpu_to_le32(~0xFF);
-
-		/* return some buffers to hardware, one at a time is too slow */
-		if (unlikely(cleaned_count >= E1000_RX_BUFFER_WRITE)) {
-			adapter->alloc_rx_buf(rx_ring, cleaned_count,
-					      GFP_ATOMIC);
-			cleaned_count = 0;
-		}
-
-		/* use prefetched values */
-		rx_desc = next_rxd;
-		buffer_info = next_buffer;
-
-		staterr = le32_to_cpu(rx_desc->wb.upper.status_error);
-	}
-	rx_ring->next_to_clean = i;
-
-	cleaned_count = e1000_desc_unused(rx_ring);
-	if (cleaned_count)
-		adapter->alloc_rx_buf(rx_ring, cleaned_count, GFP_ATOMIC);
-
-	adapter->total_rx_bytes += total_rx_bytes;
-	adapter->total_rx_packets += total_rx_packets;
-	return cleaned;
-}
 
 /**
  * e1000_clean_rx_ring - Free Rx Buffers per Queue
@@ -2476,128 +1813,6 @@ void e1000e_free_rx_resources(struct e1000_ring *rx_ring)
 	rx_ring->desc = NULL;
 }
 
-/**
- * e1000_update_itr - update the dynamic ITR value based on statistics
- * @itr_setting: current adapter->itr
- * @packets: the number of packets during this measurement interval
- * @bytes: the number of bytes during this measurement interval
- *
- *      Stores a new ITR value based on packets and byte
- *      counts during the last interrupt.  The advantage of per interrupt
- *      computation is faster updates and more accurate ITR for the current
- *      traffic pattern.  Constants in this function were computed
- *      based on theoretical maximum wire speed and thresholds were set based
- *      on testing data as well as attempting to minimize response time
- *      while increasing bulk throughput.  This functionality is controlled
- *      by the InterruptThrottleRate module parameter.
- **/
-static unsigned int e1000_update_itr(u16 itr_setting, int packets, int bytes)
-{
-	unsigned int retval = itr_setting;
-
-	if (packets == 0)
-		return itr_setting;
-
-	switch (itr_setting) {
-	case lowest_latency:
-		/* handle TSO and jumbo frames */
-		if (bytes / packets > 8000)
-			retval = bulk_latency;
-		else if ((packets < 5) && (bytes > 512))
-			retval = low_latency;
-		break;
-	case low_latency:	/* 50 usec aka 20000 ints/s */
-		if (bytes > 10000) {
-			/* this if handles the TSO accounting */
-			if (bytes / packets > 8000)
-				retval = bulk_latency;
-			else if ((packets < 10) || ((bytes / packets) > 1200))
-				retval = bulk_latency;
-			else if ((packets > 35))
-				retval = lowest_latency;
-		} else if (bytes / packets > 2000) {
-			retval = bulk_latency;
-		} else if (packets <= 2 && bytes < 512) {
-			retval = lowest_latency;
-		}
-		break;
-	case bulk_latency:	/* 250 usec aka 4000 ints/s */
-		if (bytes > 25000) {
-			if (packets > 35)
-				retval = low_latency;
-		} else if (bytes < 6000) {
-			retval = low_latency;
-		}
-		break;
-	}
-
-	return retval;
-}
-
-static void e1000_set_itr(struct e1000_adapter *adapter)
-{
-	u16 current_itr;
-	u32 new_itr = adapter->itr;
-
-	/* for non-gigabit speeds, just fix the interrupt rate at 4000 */
-	if (adapter->link_speed != SPEED_1000) {
-		new_itr = 4000;
-		goto set_itr_now;
-	}
-
-	if (adapter->flags2 & FLAG2_DISABLE_AIM) {
-		new_itr = 0;
-		goto set_itr_now;
-	}
-
-	adapter->tx_itr = e1000_update_itr(adapter->tx_itr,
-					   adapter->total_tx_packets,
-					   adapter->total_tx_bytes);
-	/* conservative mode (itr 3) eliminates the lowest_latency setting */
-	if (adapter->itr_setting == 3 && adapter->tx_itr == lowest_latency)
-		adapter->tx_itr = low_latency;
-
-	adapter->rx_itr = e1000_update_itr(adapter->rx_itr,
-					   adapter->total_rx_packets,
-					   adapter->total_rx_bytes);
-	/* conservative mode (itr 3) eliminates the lowest_latency setting */
-	if (adapter->itr_setting == 3 && adapter->rx_itr == lowest_latency)
-		adapter->rx_itr = low_latency;
-
-	current_itr = max(adapter->rx_itr, adapter->tx_itr);
-
-	/* counts and packets in update_itr are dependent on these numbers */
-	switch (current_itr) {
-	case lowest_latency:
-		new_itr = 70000;
-		break;
-	case low_latency:
-		new_itr = 20000;	/* aka hwitr = ~200 */
-		break;
-	case bulk_latency:
-		new_itr = 4000;
-		break;
-	default:
-		break;
-	}
-
-set_itr_now:
-	if (new_itr != adapter->itr) {
-		/* this attempts to bias the interrupt rate towards Bulk
-		 * by adding intermediate steps when interrupt rate is
-		 * increasing
-		 */
-		new_itr = new_itr > adapter->itr ?
-		    min(adapter->itr + (new_itr >> 2), new_itr) : new_itr;
-		adapter->itr = new_itr;
-		adapter->rx_ring->itr_val = new_itr;
-		if (adapter->msix_entries)
-			adapter->rx_ring->set_itr = 1;
-		else
-			e1000e_write_itr(adapter, new_itr);
-	}
-}
-
 /**
  * e1000e_write_itr - write the ITR value to the appropriate registers
  * @adapter: address of board private structure
@@ -2650,47 +1865,6 @@ err:
 	return -ENOMEM;
 }
 
-/**
- * e1000e_poll - NAPI Rx polling callback
- * @napi: struct associated with this polling callback
- * @budget: number of packets driver is allowed to process this poll
- **/
-static int e1000e_poll(struct napi_struct *napi, int budget)
-{
-	struct e1000_adapter *adapter = container_of(napi, struct e1000_adapter,
-						     napi);
-	struct e1000_hw *hw = &adapter->hw;
-	struct net_device *poll_dev = adapter->netdev;
-	int tx_cleaned = 1, work_done = 0;
-
-	adapter = netdev_priv(poll_dev);
-
-	if (!adapter->msix_entries ||
-	    (adapter->rx_ring->ims_val & adapter->tx_ring->ims_val))
-		tx_cleaned = e1000_clean_tx_irq(adapter->tx_ring);
-
-	adapter->clean_rx(adapter->rx_ring, &work_done, budget);
-
-	if (!tx_cleaned || work_done == budget)
-		return budget;
-
-	/* Exit the polling mode, but don't re-enable interrupts if stack might
-	 * poll us due to busy-polling
-	 */
-	if (likely(napi_complete_done(napi, work_done))) {
-		if (adapter->itr_setting & 3)
-			e1000_set_itr(adapter);
-		if (!test_bit(__E1000_DOWN, &adapter->state)) {
-			if (adapter->msix_entries)
-				ew32(IMS, adapter->rx_ring->ims_val);
-			else
-				e1000_irq_enable(adapter);
-		}
-	}
-
-	return work_done;
-}
-
 static int e1000_vlan_rx_add_vid(struct net_device *netdev,
 				 __always_unused __be16 proto, u16 vid)
 {
@@ -3250,142 +2424,47 @@ static void e1000_configure_rx(struct e1000_adapter *adapter)
 	ew32(RDLEN(0), rdlen);
 	ew32(RDH(0), 0);
 	ew32(RDT(0), 0);
-	rx_ring->head = adapter->hw.hw_addr + E1000_RDH(0);
-	rx_ring->tail = adapter->hw.hw_addr + E1000_RDT(0);
-
-	writel(0, rx_ring->head);
-	if (adapter->flags2 & FLAG2_PCIM2PCI_ARBITER_WA)
-		e1000e_update_rdt_wa(rx_ring, 0);
-	else
-		writel(0, rx_ring->tail);
-
-	/* Enable Receive Checksum Offload for TCP and UDP */
-	rxcsum = er32(RXCSUM);
-	if (adapter->netdev->features & NETIF_F_RXCSUM)
-		rxcsum |= E1000_RXCSUM_TUOFL;
-	else
-		rxcsum &= ~E1000_RXCSUM_TUOFL;
-	ew32(RXCSUM, rxcsum);
-
-	/* With jumbo frames, excessive C-state transition latencies result
-	 * in dropped transactions.
-	 */
-	if (adapter->netdev->mtu > ETH_DATA_LEN) {
-		u32 lat =
-		    ((er32(PBA) & E1000_PBA_RXA_MASK) * 1024 -
-		     adapter->max_frame_size) * 8 / 1000;
-
-		if (adapter->flags & FLAG_IS_ICH) {
-			u32 rxdctl = er32(RXDCTL(0));
-
-			ew32(RXDCTL(0), rxdctl | 0x3 | BIT(8));
-		}
-
-		dev_info(&adapter->pdev->dev,
-			 "Some CPU C-states have been disabled in order to enable jumbo frames\n");
-		cpu_latency_qos_update_request(&adapter->pm_qos_req, lat);
-	} else {
-		cpu_latency_qos_update_request(&adapter->pm_qos_req,
-					       PM_QOS_DEFAULT_VALUE);
-	}
-
-	/* Enable Receives */
-	ew32(RCTL, rctl);
-}
-
-/**
- * e1000e_write_mc_addr_list - write multicast addresses to MTA
- * @netdev: network interface device structure
- *
- * Writes multicast address list to the MTA hash table.
- * Returns: -ENOMEM on failure
- *                0 on no addresses written
- *                X on writing X addresses to MTA
- */
-static int e1000e_write_mc_addr_list(struct net_device *netdev)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	struct e1000_hw *hw = &adapter->hw;
-	struct netdev_hw_addr *ha;
-	u8 *mta_list;
-	int i;
-
-	if (netdev_mc_empty(netdev)) {
-		/* nothing to program, so clear mc list */
-		hw->mac.ops.update_mc_addr_list(hw, NULL, 0);
-		return 0;
-	}
-
-	mta_list = kcalloc(netdev_mc_count(netdev), ETH_ALEN, GFP_ATOMIC);
-	if (!mta_list)
-		return -ENOMEM;
-
-	/* update_mc_addr_list expects a packed array of only addresses. */
-	i = 0;
-	netdev_for_each_mc_addr(ha, netdev)
-	    memcpy(mta_list + (i++ * ETH_ALEN), ha->addr, ETH_ALEN);
-
-	hw->mac.ops.update_mc_addr_list(hw, mta_list, i);
-	kfree(mta_list);
-
-	return netdev_mc_count(netdev);
-}
-
-/**
- * e1000e_write_uc_addr_list - write unicast addresses to RAR table
- * @netdev: network interface device structure
- *
- * Writes unicast address list to the RAR table.
- * Returns: -ENOMEM on failure/insufficient address space
- *                0 on no addresses written
- *                X on writing X addresses to the RAR table
- **/
-static int e1000e_write_uc_addr_list(struct net_device *netdev)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	struct e1000_hw *hw = &adapter->hw;
-	unsigned int rar_entries;
-	int count = 0;
-
-	rar_entries = hw->mac.ops.rar_get_count(hw);
-
-	/* save a rar entry for our hardware address */
-	rar_entries--;
+	rx_ring->head = adapter->hw.hw_addr + E1000_RDH(0);
+	rx_ring->tail = adapter->hw.hw_addr + E1000_RDT(0);
 
-	/* save a rar entry for the LAA workaround */
-	if (adapter->flags & FLAG_RESET_OVERWRITES_LAA)
-		rar_entries--;
+	writel(0, rx_ring->head);
+	if (adapter->flags2 & FLAG2_PCIM2PCI_ARBITER_WA)
+		e1000e_update_rdt_wa(rx_ring, 0);
+	else
+		writel(0, rx_ring->tail);
 
-	/* return ENOMEM indicating insufficient memory for addresses */
-	if (netdev_uc_count(netdev) > rar_entries)
-		return -ENOMEM;
+	/* Enable Receive Checksum Offload for TCP and UDP */
+	rxcsum = er32(RXCSUM);
+	if (adapter->netdev->features & NETIF_F_RXCSUM)
+		rxcsum |= E1000_RXCSUM_TUOFL;
+	else
+		rxcsum &= ~E1000_RXCSUM_TUOFL;
+	ew32(RXCSUM, rxcsum);
 
-	if (!netdev_uc_empty(netdev) && rar_entries) {
-		struct netdev_hw_addr *ha;
+	/* With jumbo frames, excessive C-state transition latencies result
+	 * in dropped transactions.
+	 */
+	if (adapter->netdev->mtu > ETH_DATA_LEN) {
+		u32 lat =
+		    ((er32(PBA) & E1000_PBA_RXA_MASK) * 1024 -
+		     adapter->max_frame_size) * 8 / 1000;
 
-		/* write the addresses in reverse order to avoid write
-		 * combining
-		 */
-		netdev_for_each_uc_addr(ha, netdev) {
-			int ret_val;
+		if (adapter->flags & FLAG_IS_ICH) {
+			u32 rxdctl = er32(RXDCTL(0));
 
-			if (!rar_entries)
-				break;
-			ret_val = hw->mac.ops.rar_set(hw, ha->addr, rar_entries--);
-			if (ret_val < 0)
-				return -ENOMEM;
-			count++;
+			ew32(RXDCTL(0), rxdctl | 0x3 | BIT(8));
 		}
-	}
 
-	/* zero out the remaining RAR entries not used above */
-	for (; rar_entries > 0; rar_entries--) {
-		ew32(RAH(rar_entries), 0);
-		ew32(RAL(rar_entries), 0);
+		dev_info(&adapter->pdev->dev,
+			 "Some CPU C-states have been disabled in order to enable jumbo frames\n");
+		cpu_latency_qos_update_request(&adapter->pm_qos_req, lat);
+	} else {
+		cpu_latency_qos_update_request(&adapter->pm_qos_req,
+					       PM_QOS_DEFAULT_VALUE);
 	}
-	e1e_flush();
 
-	return count;
+	/* Enable Receives */
+	ew32(RCTL, rctl);
 }
 
 /**
@@ -4761,45 +3840,6 @@ int e1000e_close(struct net_device *netdev)
 	return 0;
 }
 
-/**
- * e1000_set_mac - Change the Ethernet Address of the NIC
- * @netdev: network interface device structure
- * @p: pointer to an address structure
- *
- * Returns 0 on success, negative on failure
- **/
-static int e1000_set_mac(struct net_device *netdev, void *p)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	struct e1000_hw *hw = &adapter->hw;
-	struct sockaddr *addr = p;
-
-	if (!is_valid_ether_addr(addr->sa_data))
-		return -EADDRNOTAVAIL;
-
-	eth_hw_addr_set(netdev, addr->sa_data);
-	memcpy(adapter->hw.mac.addr, addr->sa_data, netdev->addr_len);
-
-	hw->mac.ops.rar_set(&adapter->hw, adapter->hw.mac.addr, 0);
-
-	if (adapter->flags & FLAG_RESET_OVERWRITES_LAA) {
-		/* activate the work around */
-		e1000e_set_laa_state_82571(&adapter->hw, 1);
-
-		/* Hold a copy of the LAA in RAR[14] This is done so that
-		 * between the time RAR[0] gets clobbered  and the time it
-		 * gets fixed (in e1000_watchdog), the actual LAA is in one
-		 * of the RARs and no incoming packets directed to this port
-		 * are dropped. Eventually the LAA will be in RAR[0] and
-		 * RAR[14]
-		 */
-		hw->mac.ops.rar_set(&adapter->hw, adapter->hw.mac.addr,
-				    adapter->hw.mac.rar_entry_count - 1);
-	}
-
-	return 0;
-}
-
 /**
  * e1000e_update_phy_task - work thread to update phy
  * @work: pointer to our work struct
@@ -5424,125 +4464,6 @@ link_up:
 #define E1000_TX_FLAGS_VLAN_MASK	0xffff0000
 #define E1000_TX_FLAGS_VLAN_SHIFT	16
 
-static int e1000_tso(struct e1000_ring *tx_ring, struct sk_buff *skb,
-		     __be16 protocol)
-{
-	struct e1000_context_desc *context_desc;
-	struct e1000_buffer *buffer_info;
-	unsigned int i;
-	u32 cmd_length = 0;
-	u16 ipcse = 0, mss;
-	u8 ipcss, ipcso, tucss, tucso, hdr_len;
-	int err;
-
-	if (!skb_is_gso(skb))
-		return 0;
-
-	err = skb_cow_head(skb, 0);
-	if (err < 0)
-		return err;
-
-	hdr_len = skb_tcp_all_headers(skb);
-	mss = skb_shinfo(skb)->gso_size;
-	if (protocol == htons(ETH_P_IP)) {
-		struct iphdr *iph = ip_hdr(skb);
-		iph->tot_len = 0;
-		iph->check = 0;
-		tcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr,
-							 0, IPPROTO_TCP, 0);
-		cmd_length = E1000_TXD_CMD_IP;
-		ipcse = skb_transport_offset(skb) - 1;
-	} else if (skb_is_gso_v6(skb)) {
-		tcp_v6_gso_csum_prep(skb);
-		ipcse = 0;
-	}
-	ipcss = skb_network_offset(skb);
-	ipcso = (void *)&(ip_hdr(skb)->check) - (void *)skb->data;
-	tucss = skb_transport_offset(skb);
-	tucso = (void *)&(tcp_hdr(skb)->check) - (void *)skb->data;
-
-	cmd_length |= (E1000_TXD_CMD_DEXT | E1000_TXD_CMD_TSE |
-		       E1000_TXD_CMD_TCP | (skb->len - (hdr_len)));
-
-	i = tx_ring->next_to_use;
-	context_desc = E1000_CONTEXT_DESC(*tx_ring, i);
-	buffer_info = &tx_ring->buffer_info[i];
-
-	context_desc->lower_setup.ip_fields.ipcss = ipcss;
-	context_desc->lower_setup.ip_fields.ipcso = ipcso;
-	context_desc->lower_setup.ip_fields.ipcse = cpu_to_le16(ipcse);
-	context_desc->upper_setup.tcp_fields.tucss = tucss;
-	context_desc->upper_setup.tcp_fields.tucso = tucso;
-	context_desc->upper_setup.tcp_fields.tucse = 0;
-	context_desc->tcp_seg_setup.fields.mss = cpu_to_le16(mss);
-	context_desc->tcp_seg_setup.fields.hdr_len = hdr_len;
-	context_desc->cmd_and_length = cpu_to_le32(cmd_length);
-
-	buffer_info->time_stamp = jiffies;
-	buffer_info->next_to_watch = i;
-
-	i++;
-	if (i == tx_ring->count)
-		i = 0;
-	tx_ring->next_to_use = i;
-
-	return 1;
-}
-
-static bool e1000_tx_csum(struct e1000_ring *tx_ring, struct sk_buff *skb,
-			  __be16 protocol)
-{
-	struct e1000_adapter *adapter = tx_ring->adapter;
-	struct e1000_context_desc *context_desc;
-	struct e1000_buffer *buffer_info;
-	unsigned int i;
-	u8 css;
-	u32 cmd_len = E1000_TXD_CMD_DEXT;
-
-	if (skb->ip_summed != CHECKSUM_PARTIAL)
-		return false;
-
-	switch (protocol) {
-	case cpu_to_be16(ETH_P_IP):
-		if (ip_hdr(skb)->protocol == IPPROTO_TCP)
-			cmd_len |= E1000_TXD_CMD_TCP;
-		break;
-	case cpu_to_be16(ETH_P_IPV6):
-		/* XXX not handling all IPV6 headers */
-		if (ipv6_hdr(skb)->nexthdr == IPPROTO_TCP)
-			cmd_len |= E1000_TXD_CMD_TCP;
-		break;
-	default:
-		if (unlikely(net_ratelimit()))
-			e_warn("checksum_partial proto=%x!\n",
-			       be16_to_cpu(protocol));
-		break;
-	}
-
-	css = skb_checksum_start_offset(skb);
-
-	i = tx_ring->next_to_use;
-	buffer_info = &tx_ring->buffer_info[i];
-	context_desc = E1000_CONTEXT_DESC(*tx_ring, i);
-
-	context_desc->lower_setup.ip_config = 0;
-	context_desc->upper_setup.tcp_fields.tucss = css;
-	context_desc->upper_setup.tcp_fields.tucso = css + skb->csum_offset;
-	context_desc->upper_setup.tcp_fields.tucse = 0;
-	context_desc->tcp_seg_setup.data = 0;
-	context_desc->cmd_and_length = cpu_to_le32(cmd_len);
-
-	buffer_info->time_stamp = jiffies;
-	buffer_info->next_to_watch = i;
-
-	i++;
-	if (i == tx_ring->count)
-		i = 0;
-	tx_ring->next_to_use = i;
-
-	return true;
-}
-
 static int e1000_tx_map(struct e1000_ring *tx_ring, struct sk_buff *skb,
 			unsigned int first, unsigned int max_per_txd,
 			unsigned int nr_frags)
@@ -5743,38 +4664,6 @@ static int e1000_transfer_dhcp_info(struct e1000_adapter *adapter,
 	return 0;
 }
 
-static int __e1000_maybe_stop_tx(struct e1000_ring *tx_ring, int size)
-{
-	struct e1000_adapter *adapter = tx_ring->adapter;
-
-	netif_stop_queue(adapter->netdev);
-	/* Herbert's original patch had:
-	 *  smp_mb__after_netif_stop_queue();
-	 * but since that doesn't exist yet, just open code it.
-	 */
-	smp_mb();
-
-	/* We need to check again in a case another CPU has just
-	 * made room available.
-	 */
-	if (e1000_desc_unused(tx_ring) < size)
-		return -EBUSY;
-
-	/* A reprieve! */
-	netif_start_queue(adapter->netdev);
-	++adapter->restart_queue;
-	return 0;
-}
-
-static int e1000_maybe_stop_tx(struct e1000_ring *tx_ring, int size)
-{
-	BUG_ON(size > tx_ring->count);
-
-	if (e1000_desc_unused(tx_ring) >= size)
-		return 0;
-	return __e1000_maybe_stop_tx(tx_ring, size);
-}
-
 static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 				    struct net_device *netdev)
 {
@@ -5925,20 +4814,6 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 	return NETDEV_TX_OK;
 }
 
-/**
- * e1000_tx_timeout - Respond to a Tx Hang
- * @netdev: network interface device structure
- * @txqueue: index of the hung queue (unused)
- **/
-static void e1000_tx_timeout(struct net_device *netdev, unsigned int __always_unused txqueue)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-
-	/* Do the reset outside of interrupt context */
-	adapter->tx_timeout_count++;
-	schedule_work(&adapter->reset_task);
-}
-
 static void e1000_reset_task(struct work_struct *work)
 {
 	struct e1000_adapter *adapter;
@@ -6005,133 +4880,6 @@ void e1000e_get_stats64(struct net_device *netdev,
 	spin_unlock(&adapter->stats64_lock);
 }
 
-/**
- * e1000_change_mtu - Change the Maximum Transfer Unit
- * @netdev: network interface device structure
- * @new_mtu: new value for maximum frame size
- *
- * Returns 0 on success, negative on failure
- **/
-static int e1000_change_mtu(struct net_device *netdev, int new_mtu)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	int max_frame = new_mtu + VLAN_ETH_HLEN + ETH_FCS_LEN;
-
-	/* Jumbo frame support */
-	if ((new_mtu > ETH_DATA_LEN) &&
-	    !(adapter->flags & FLAG_HAS_JUMBO_FRAMES)) {
-		e_err("Jumbo Frames not supported.\n");
-		return -EINVAL;
-	}
-
-	/* Jumbo frame workaround on 82579 and newer requires CRC be stripped */
-	if ((adapter->hw.mac.type >= e1000_pch2lan) &&
-	    !(adapter->flags2 & FLAG2_CRC_STRIPPING) &&
-	    (new_mtu > ETH_DATA_LEN)) {
-		e_err("Jumbo Frames not supported on this device when CRC stripping is disabled.\n");
-		return -EINVAL;
-	}
-
-	while (test_and_set_bit(__E1000_RESETTING, &adapter->state))
-		usleep_range(1000, 1100);
-	/* e1000e_down -> e1000e_reset dependent on max_frame_size & mtu */
-	adapter->max_frame_size = max_frame;
-	netdev_dbg(netdev, "changing MTU from %d to %d\n",
-		   netdev->mtu, new_mtu);
-	netdev->mtu = new_mtu;
-
-	pm_runtime_get_sync(netdev->dev.parent);
-
-	if (netif_running(netdev))
-		e1000e_down(adapter, true);
-
-	/* NOTE: netdev_alloc_skb reserves 16 bytes, and typically NET_IP_ALIGN
-	 * means we reserve 2 more, this pushes us to allocate from the next
-	 * larger slab size.
-	 * i.e. RXBUFFER_2048 --> size-4096 slab
-	 * However with the new *_jumbo_rx* routines, jumbo receives will use
-	 * fragmented skbs
-	 */
-
-	if (max_frame <= 2048)
-		adapter->rx_buffer_len = 2048;
-	else
-		adapter->rx_buffer_len = 4096;
-
-	/* adjust allocation if LPE protects us, and we aren't using SBP */
-	if (max_frame <= (VLAN_ETH_FRAME_LEN + ETH_FCS_LEN))
-		adapter->rx_buffer_len = VLAN_ETH_FRAME_LEN + ETH_FCS_LEN;
-
-	if (netif_running(netdev))
-		e1000e_up(adapter);
-	else
-		e1000e_reset(adapter);
-
-	pm_runtime_put_sync(netdev->dev.parent);
-
-	clear_bit(__E1000_RESETTING, &adapter->state);
-
-	return 0;
-}
-
-static int e1000_mii_ioctl(struct net_device *netdev, struct ifreq *ifr,
-			   int cmd)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	struct mii_ioctl_data *data = if_mii(ifr);
-
-	if (adapter->hw.phy.media_type != e1000_media_type_copper)
-		return -EOPNOTSUPP;
-
-	switch (cmd) {
-	case SIOCGMIIPHY:
-		data->phy_id = adapter->hw.phy.addr;
-		break;
-	case SIOCGMIIREG:
-		e1000_phy_read_status(adapter);
-
-		switch (data->reg_num & 0x1F) {
-		case MII_BMCR:
-			data->val_out = adapter->phy_regs.bmcr;
-			break;
-		case MII_BMSR:
-			data->val_out = adapter->phy_regs.bmsr;
-			break;
-		case MII_PHYSID1:
-			data->val_out = (adapter->hw.phy.id >> 16);
-			break;
-		case MII_PHYSID2:
-			data->val_out = (adapter->hw.phy.id & 0xFFFF);
-			break;
-		case MII_ADVERTISE:
-			data->val_out = adapter->phy_regs.advertise;
-			break;
-		case MII_LPA:
-			data->val_out = adapter->phy_regs.lpa;
-			break;
-		case MII_EXPANSION:
-			data->val_out = adapter->phy_regs.expansion;
-			break;
-		case MII_CTRL1000:
-			data->val_out = adapter->phy_regs.ctrl1000;
-			break;
-		case MII_STAT1000:
-			data->val_out = adapter->phy_regs.stat1000;
-			break;
-		case MII_ESTATUS:
-			data->val_out = adapter->phy_regs.estatus;
-			break;
-		default:
-			return -EIO;
-		}
-		break;
-	case SIOCSMIIREG:
-	default:
-		return -EOPNOTSUPP;
-	}
-	return 0;
-}
-
 /**
  * e1000e_hwtstamp_set - control hardware time stamping
  * @netdev: network interface device structure
@@ -6191,22 +4939,6 @@ static int e1000e_hwtstamp_get(struct net_device *netdev, struct ifreq *ifr)
 			    sizeof(adapter->hwtstamp_config)) ? -EFAULT : 0;
 }
 
-static int e1000_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
-{
-	switch (cmd) {
-	case SIOCGMIIPHY:
-	case SIOCGMIIREG:
-	case SIOCSMIIREG:
-		return e1000_mii_ioctl(netdev, ifr, cmd);
-	case SIOCSHWTSTAMP:
-		return e1000e_hwtstamp_set(netdev, ifr);
-	case SIOCGHWTSTAMP:
-		return e1000e_hwtstamp_get(netdev, ifr);
-	default:
-		return -EOPNOTSUPP;
-	}
-}
-
 static int e1000_init_phy_wakeup(struct e1000_adapter *adapter, u32 wufc)
 {
 	struct e1000_hw *hw = &adapter->hw;
@@ -6281,28 +5013,6 @@ release:
 	return retval;
 }
 
-static void e1000e_flush_lpic(struct pci_dev *pdev)
-{
-	struct net_device *netdev = pci_get_drvdata(pdev);
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	struct e1000_hw *hw = &adapter->hw;
-	u32 ret_val;
-
-	pm_runtime_get_sync(netdev->dev.parent);
-
-	ret_val = hw->phy.ops.acquire(hw);
-	if (ret_val)
-		goto fl_out;
-
-	pr_info("EEE TX LPI TIMER: %08X\n",
-		er32(LPIC) >> E1000_LPIC_LPIET_SHIFT);
-
-	hw->phy.ops.release(hw);
-
-fl_out:
-	pm_runtime_put_sync(netdev->dev.parent);
-}
-
 /* S0ix implementation */
 static void e1000e_s0ix_entry_flow(struct e1000_adapter *adapter)
 {
@@ -6981,8 +5691,6 @@ static __maybe_unused int e1000e_pm_suspend(struct device *dev)
 	struct pci_dev *pdev = to_pci_dev(dev);
 	int rc;
 
-	e1000e_flush_lpic(pdev);
-
 	e1000e_pm_freeze(dev);
 
 	rc = __e1000_shutdown(pdev, false);
@@ -7078,73 +5786,12 @@ static __maybe_unused int e1000e_pm_runtime_suspend(struct device *dev)
 
 static void e1000_shutdown(struct pci_dev *pdev)
 {
-	e1000e_flush_lpic(pdev);
-
 	e1000e_pm_freeze(&pdev->dev);
 
 	__e1000_shutdown(pdev, false);
 }
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
-
-static irqreturn_t e1000_intr_msix(int __always_unused irq, void *data)
-{
-	struct net_device *netdev = data;
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-
-	if (adapter->msix_entries) {
-		int vector, msix_irq;
-
-		vector = 0;
-		msix_irq = adapter->msix_entries[vector].vector;
-		if (disable_hardirq(msix_irq))
-			e1000_intr_msix_rx(msix_irq, netdev);
-		enable_irq(msix_irq);
-
-		vector++;
-		msix_irq = adapter->msix_entries[vector].vector;
-		if (disable_hardirq(msix_irq))
-			e1000_intr_msix_tx(msix_irq, netdev);
-		enable_irq(msix_irq);
-
-		vector++;
-		msix_irq = adapter->msix_entries[vector].vector;
-		if (disable_hardirq(msix_irq))
-			e1000_msix_other(msix_irq, netdev);
-		enable_irq(msix_irq);
-	}
-
-	return IRQ_HANDLED;
-}
-
-/**
- * e1000_netpoll
- * @netdev: network interface device structure
- *
- * Polling 'interrupt' - used by things like netconsole to send skbs
- * without having to re-enable interrupts. It's not called while
- * the interrupt routine is executing.
- */
-static void e1000_netpoll(struct net_device *netdev)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-
-	switch (adapter->int_mode) {
-	case E1000E_INT_MODE_MSIX:
-		e1000_intr_msix(adapter->pdev->irq, netdev);
-		break;
-	case E1000E_INT_MODE_MSI:
-		if (disable_hardirq(adapter->pdev->irq))
-			e1000_intr_msi(adapter->pdev->irq, netdev);
-		enable_irq(adapter->pdev->irq);
-		break;
-	default:		/* E1000E_INT_MODE_LEGACY */
-		if (disable_hardirq(adapter->pdev->irq))
-			e1000_intr(adapter->pdev->irq, netdev);
-		enable_irq(adapter->pdev->irq);
-		break;
-	}
-}
 #endif
 
 /**
@@ -7280,65 +5927,6 @@ static void e1000_eeprom_checks(struct e1000_adapter *adapter)
 	}
 }
 
-static netdev_features_t e1000_fix_features(struct net_device *netdev,
-					    netdev_features_t features)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	struct e1000_hw *hw = &adapter->hw;
-
-	/* Jumbo frame workaround on 82579 and newer requires CRC be stripped */
-	if ((hw->mac.type >= e1000_pch2lan) && (netdev->mtu > ETH_DATA_LEN))
-		features &= ~NETIF_F_RXFCS;
-
-	/* Since there is no support for separate Rx/Tx vlan accel
-	 * enable/disable make sure Tx flag is always in same state as Rx.
-	 */
-	if (features & NETIF_F_HW_VLAN_CTAG_RX)
-		features |= NETIF_F_HW_VLAN_CTAG_TX;
-	else
-		features &= ~NETIF_F_HW_VLAN_CTAG_TX;
-
-	return features;
-}
-
-static int e1000_set_features(struct net_device *netdev,
-			      netdev_features_t features)
-{
-	struct e1000_adapter *adapter = netdev_priv(netdev);
-	netdev_features_t changed = features ^ netdev->features;
-
-	if (changed & (NETIF_F_TSO | NETIF_F_TSO6))
-		adapter->flags |= FLAG_TSO_FORCE;
-
-	if (!(changed & (NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_TX |
-			 NETIF_F_RXCSUM | NETIF_F_RXHASH | NETIF_F_RXFCS |
-			 NETIF_F_RXALL)))
-		return 0;
-
-	if (changed & NETIF_F_RXFCS) {
-		if (features & NETIF_F_RXFCS) {
-			adapter->flags2 &= ~FLAG2_CRC_STRIPPING;
-		} else {
-			/* We need to take it back to defaults, which might mean
-			 * stripping is still disabled at the adapter level.
-			 */
-			if (adapter->flags2 & FLAG2_DFLT_CRC_STRIPPING)
-				adapter->flags2 |= FLAG2_CRC_STRIPPING;
-			else
-				adapter->flags2 &= ~FLAG2_CRC_STRIPPING;
-		}
-	}
-
-	netdev->features = features;
-
-	if (netif_running(netdev))
-		e1000e_reinit_locked(adapter);
-	else
-		e1000e_reset(adapter);
-
-	return 1;
-}
-
 static const struct net_device_ops e1000e_netdev_ops = {
 	.ndo_open		= e1000e_open,
 	.ndo_stop		= e1000e_close,
-- 
2.39.2

